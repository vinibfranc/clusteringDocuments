{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "\n",
    "#DIRETÓRIO COM AS PASTAS DOS GRUPOS E OS DOCUMENTOS DENTRO\n",
    "DIR = \"C:\\\\Users\\\\Marcos\\\\Downloads\\\\AllDocs\\\\\"\n",
    "\n",
    "groups = {}\n",
    "\n",
    "#CRIA O DICIONÁRIO COM UM VETOR VAZIO AONDE VAI SER INSERIDO OS DOCUMENTOS DE CADA GRUPO\n",
    "for subDirName in os.listdir(DIR):\n",
    "    groups[subDirName] = []\n",
    "    \n",
    "\n",
    "#ADICIONA OS DOCUMENTOS EM CADA RESPECTIVO GRUPO\n",
    "for group in groups:\n",
    "    for document in os.listdir(DIR + group + \"\\\\\"):\n",
    "        file = open(DIR + group + \"\\\\\" + document, \"r\")\n",
    "        groups[group].append(file.read())\n",
    "        \n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens: stems.append(PorterStemmer().stem(item))\n",
    "    spacedStems = \" \".join(stems)\n",
    "    return spacedStems\n",
    "  \n",
    "#TOKENIZA TODOS OS DOCUMENTOS DE TODOS OS GRUPOS    \n",
    "for group in groups:\n",
    "    for i in range(len(groups[group])):\n",
    "        groups[group][i] = tokenize(groups[group][i])\n",
    "\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union([\"book\"])\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=my_stop_words)\n",
    "\n",
    "documents_together = []\n",
    "for group in groups:\n",
    "    for i in range(len(groups[group])):\n",
    "        documents_together.append(groups[group][i])\n",
    "        \n",
    "final_top_words = {}\n",
    "\n",
    "for group in groups:\n",
    "    matrix = vectorizer.fit_transform(groups[group]).todense()\n",
    "    matrix = pd.DataFrame(matrix, columns=vectorizer.get_feature_names())\n",
    "    top_words = matrix.sum(axis=0).sort_values(ascending=False)\n",
    "    final_top_words[group] = (top_words[0:15]).keys()\n",
    "    \n",
    "top_words_grouped = []\n",
    "for group in final_top_words:\n",
    "    top_words_grouped.append(final_top_words[group][0:len(final_top_words[group])])\n",
    "\n",
    "top_words_together = []\n",
    "for i in range(0, len(np.array(top_words_grouped))):\n",
    "    for word in np.array(top_words_grouped)[i]:\n",
    "        top_words_together.append(word)\n",
    "            \n",
    "top_words_together_unique = []\n",
    "top_words_together_unique = list(set(top_words_together))\n",
    "\n",
    "#TF\n",
    "tf = []\n",
    "for d in range(0,len(documents_together)):\n",
    "    doc = []\n",
    "    for word in top_words_together_unique:\n",
    "        doc.append(documents_together[i].count(word) / len(documents_together[i]))\n",
    "    tf.append(doc)\n",
    "    \n",
    "#IDF\n",
    "df = []\n",
    "for d in range(0,len(documents_together)):\n",
    "    doc = []\n",
    "    for w in range(0,len(top_words_together_unique)):\n",
    "        doc.append(0)  \n",
    "    df.append(doc)\n",
    "    \n",
    "\n",
    "for d in range(0,len(documents_together)):\n",
    "    for w in range(0,len(top_words_together_unique)):\n",
    "        if(documents_together[d].count(top_words_together_unique[w]) > 0):\n",
    "            for d in range(0,len(documents_together)):\n",
    "                df[d][w] += 1\n",
    "\n",
    "\n",
    "idf = []\n",
    "for d in range(0,len(documents_together)):\n",
    "    doc = []\n",
    "    for w in range(0,len(top_words_together_unique)):\n",
    "        if(df[d][w] == 0):\n",
    "            doc.append(0)\n",
    "        else:\n",
    "            doc.append(np.log(len(documents_together)/df[d][w]))\n",
    "    idf.append(doc)\n",
    "    \n",
    "tfidf = []\n",
    "for i in range(0, len(idf)):\n",
    "    doc = []\n",
    "    for z in range(len(idf[i])):\n",
    "        doc.append(idf[i][z] * df[i][z])\n",
    "    tfidf.append(doc)\n",
    "    \n",
    "print(len(tfidf[0]))\n",
    "kmeans = KMeans(n_clusters = 7, init = 'random', max_iter = 400)\n",
    "kmeans.fit(tfidf)\n",
    "y_kmeans = kmeans.predict(tfidf)\n",
    "\n",
    "print(y_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
